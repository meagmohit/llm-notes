---
layout: single
classes: wide
title: Reading List
permalink: /docs/reading-list/
last_modified_at: 2025-01-17T17:00:00-08:00s
# toc: true
---

## Transformers


| Title                                                                        |  Topic       |   Comments                                   |
| --------------------------------------------------------------------------   | ------------ | -------------------------------------------- |
| [Transformers Primer by Aman.AI](https://aman.ai/primers/ai/transformers/) {% cite Chadha2020DistilledTransformers%}| Transformers | Very comprehensive                                           |
| [The Illustrated Transformer by Jay Alammar](https://jalammar.github.io/illustrated-transformer/) {% cite Chadha2020DistilledTransformers%}| Transformers | Great Illustrations 
| [Attention in transformers, visually explained by 3Blue1Brown](https://www.youtube.com/watch?v=eMlx5fFNoYc&ab_channel=3Blue1Brown) {% cite 3b1b-transformers%}| Transformers | Great Visuals and Explanation
| [Some Intuition on Attention and the Transformer by Eugene Yan](https://eugeneyan.com/writing/attention/) {% cite yan2023attention%}| Transformers | Great Visuals and Explanation
| [The Transformer Family by Lilian Weng](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/) {% cite weng2020transformer%}| Advances in Transformers | Advanced transformer post-enhancements 
| [The Transformer Family 2.0 by Lilian Weng](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/) {% cite weng2023transformer%}| Advances in Transformers | Update to {% cite weng2020transformer%} the transformer family. Adds a lot of other updates on the transformers, however, some modules (which were not covered in {% cite weng2020transformer%} since itâ€™s very detailed and niche. 
| --------------------------------------------------------------------------   | ------------ | -------------------------------------------- |
| [The Illustrated BERT](https://jalammar.github.io/illustrated-bert/) {% cite alammar-illustratedtbert%}| LMs | Good Short Overview
| [Generalized Language Models by Lilian Weng](https://lilianweng.github.io/posts/2019-01-31-lm/) {% cite weng2019LM%}| LMs | Great overview of BERT and its successors
| [Ten Noteworthy AI Research Papers of 2023 by Sebastian Raschka](https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023) {% cite seb-10aipapers2023%}| LMs/Research | Decent samplers of 2023 10 papers
| [AI and Open Source in 2023](https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023) {% cite seb-ai-opensource%}| LMs/Research | Decent samplers of 2023 10 papers
| [New LLM Pre-training and Post-training Paradigms](https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training) {% cite seb-new-llm-pretraining-posttraining%}| LMs/Training/Research | detailed overview of pre-training pipelines
| --------------------------------------------------------------------------   | ------------ | -------------------------------------------- |
| [Multimodality and Large Multimodal Models (LMMs) by Chip Huyen](https://huyenchip.com/2023/10/10/multimodal.html) {% cite chip-multimodal%}| MMs | Great review of MMs, with CLIP , FLAMINGO and insights
| [Generalized Visual Language Models by Lilian Weng](https://lilianweng.github.io/posts/2022-06-09-vlm/) {% cite weng2022vlm%}| MMs | Great overview of VLM techniques
| [Primers - Vision Language Models](https://aman.ai/primers/ai/vision-language-models/) {% cite Chadha2020DistilledVisionLanguageModels%}| MMs | Average Read
| --------------------------------------------------------------------------   | ------------ | -------------------------------------------- |
| [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html) {% cite chip-rlhf%}| Training | Great intro to pre-training, SFT and RMs
| [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives) {% cite seb-rlhf%}| Training | Good overview of RLHF
| [LLM Alignment by Aman.AI](https://aman.ai/primers/ai/llm-alignment/) {% cite aman-alignment%}| Training | Very thorough, not all topics are useful at the current date
| [Predictive Human Preference: From Model Ranking to Model Routing](https://huyenchip.com/2024/02/28/predictive-human-preference.html) {% cite chip-human-preference%}| Training | Basics of Model evaluation, routing and ranking. Other items like predictive human preference experiments can be ignored.
| [Aligning language models to follow instructions by OpenAI](https://openai.com/index/instruction-following/) {% cite openai-instruction-following%}| Training | 
| [Reinforcement Learning for Language Models](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81) {% cite yoavg-rl-llm%}| Training | 
| [Instruction Pretraining LLMs](https://magazine.sebastianraschka.com/p/instruction-pretraining-llms) {% cite seb-instruction-pretraining%}| Training | 
| --------------------------------------------   | ------------ | ------------------------------------------------------------ |
| [Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms) {% cite seb-finetuning-llms%}| PET Methods| Good overview of LoRA and practical tips for using it
| [Noteworthy AI Research Papers of 2024 (Part One)](https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-1) {% cite seb-aipapers-2024-part1%}| PET Methods| 6 Research Papers of 2024-H1
| [Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch) {% cite seb-improving-lora%}| PET Methods| DoRA overview in-depth
| [Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments](https://lightning.ai/pages/community/lora-insights/) {% cite seb-finetuning-llm-lightningAI%}| PET Methods| Deep-dive of {% cite seb-finetuning-llms%}
| --------------------------------------------   | ------------ | ------------------------------------------------------------ |
| [The Scaling Hypothesis](https://gwern.net/scaling-hypothesis) {% cite gwern-scaling%}| Scaling Laws | Great discussion and overview, and thought provoking (Long Read)
| [Scaling Laws in Large Language Models](https://hackernoon.com/scaling-laws-in-large-language-models) {% cite hackernoon-scaling%}| Scaling Laws | Great Quick Overview
| :==========================================:   | :==========: | :==========================================================: |
| --------------------------------------------   | ------------ | ------------------------------------------------------------ |
| [Model Merging, Mixtures of Experts, and Towards Smaller LLMs](https://magazine.sebastianraschka.com/p/research-papers-in-january-2024) {% cite seb-aipapers-2023%}| MoE/Merging | 