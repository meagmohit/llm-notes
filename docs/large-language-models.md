---
layout: single
title: Large Language Models (LLMs)
permalink: /docs/large-language-models/
toc: true
---


## Models 

### CoVE: Contextual Word Vectors (Before Transformers)

{%cite mccann2018learnedtranslationcontextualizedword%} proposed CoVE, a word embedding (function of the entire input sequence) learned by an encoder in an attentional seq-to-seq machine translation model.

{: .text-center}
![The NMT base model used in CoVe]({{ site.url }}{{ site.baseurl }}/docs/images/llm-cove.png)
Fig.  The NMT base model used in CoVe. Credits: From {% cite weng2019LM%}

### ELMo: Embeddings from Language Model

Original Paper: {%cite peters2018deepcontextualizedwordrepresentations%}, Related Papers: {%cite mccann2018learnedtranslationcontextualizedword%}
{%cite peters2017semisupervisedsequencetaggingbidirectional%}

Instead of using a fixed embedding for each word, the embeddings for the words were generated based on the context (the full sentence). ELMo used a stack of bi-directional LSTM layers, pre-trained on the language modeling task (i.e., predicting the next word). 

{: .text-center}
![ELMo Model]({{ site.url }}{{ site.baseurl }}/docs/images/llm-elmo.png)
Fig.  The contextualized word embeddings were generated by concatenating the hidden layers (from both forward and backward model), performing weighted sum over the layers based on given task. 

**Downstream Tasks**
* Semantic task: The word sense disambiguation (WSD) task emphasizes the meaning of a word given a context. The biLM top layer is better at this task than the first layer.
* Syntax task: The part-of-speech (POS) tagging task aims to infer the grammatical role of a word in one sentence. A higher accuracy can be achieved by using the biLM first layer than the top layer.

The comparison study indicates that syntactic information is better represented at lower layers while semantic information is captured by higher layers.

### Cross-View Training

In ELMo the unsupervised pre-training and task-specific learning happen for two independent models in two separate training stages. CVT {%cite clark2018semisupervisedsequencemodelingcrossview%}combines them into one unified semi-supervised learning procedure where the representation of a biLSTM encoder is improved by both supervised learning with labeled data and unsupervised learning with unlabeled data on auxiliary tasks. 

More details are in {% cite weng2019LM%} but skipped since it’s very niche.

### Generative Pre-Training (GPT) aka OpenAI Transformer
Original Paper: {%cite radford2018improving%}

* Pre-trained with language modeling tasks, using only the decoder layers (stacked 12 decoder layers)
* Used clever input transformations for different downstream tasks
* Performed supervised finetuning the base model for all downstream tasks (added the LM loss as an auxiliary loss to accelerate convergence during training, and to improve the generalization of the supervised model). 

{: .text-center}
![GPT]({{ site.url }}{{ site.baseurl }}/docs/images/llm-gpt.png)
Fig.  (left) Transformer architecture and training objectives used in this work. (right) Input
transformations for fine-tuning on different tasks. We convert all structured inputs into token
sequences to be processed by our pre-trained model, followed by a linear+softmax layer. Credits: {%cite radford2018improving%}

$$d_{model} = 768$$ and feed-forward layer dimension is 3072, 117million parameters.
**Limitation**: Unidirectional in nature, can only be used to predict the future left-to-right context

### ULMFit
Original Paper: {%cite howard2018universallanguagemodelfinetuning%}
ULM-FiT first introduced the generative pre-trained language model and task-specific fine tuning. Proposed three key ideas to achieve good transfer learning results
1. General LM pre-training: on larger wiki texts
2. Target task LM-fine tuning: Proposed two training techniques
	* Discriminative fine-tuning: Motivated by the fact that different layers of LM capture different types of information. Each layer was tuned with different learning rates, nl  for the nth layer, where n is the learning rate for the first layer.
	* Slanted triangular learning rates (STLR):  Special learning rate scheduling that first linearly increases the learning rate and then linearly decays it. The increase stage is short so that the model can converge to a parameter space suitable for the task fast, while the decay period is long allowing for better fine-tuning
3. Target task classifier fine-tuning: The pretrained LM is augmented with two standard feed-forward layers and a softmax normalization at the end to predict a target label distribution.
    * Concat pooling extracts max-polling and mean-pooling over the history of hidden states and concatenates them with the final hidden state.
	* Gradual unfreezing helps to avoid catastrophic forgetting by gradually unfreezing the model layers starting from the last one. First the last layer is unfrozen and fine-tuned for one epoch. Then the next lower layer is unfrozen. This process is repeated until all the layers are tuned.

{: .text-center}
![Three training stages of ULMFiT]({{ site.url }}{{ site.baseurl }}/docs/images/llm-ulmfit.png)
Fig. Three training stages of ULMFiT. Credits: From {% cite weng2019LM%}

### BERT
Original Paper: {%cite devlin2019bertpretrainingdeepbidirectional%}

**Key Points**
* Direct descendant of GPT, and compared to GPT main improvement is bi-directional. 
* First token was provided `[CLS]`` token, stands for classification (as in the output sequence, the first output token embedding was used for classification tasks)
* To make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?)
* Per-trained BERT can be used for creating contextualized word embeddings. The paper examines 6 different choices (which layers to choose?) specific to the downstream tasks. 

**Downstream Tasks**
* Masked Language Modeling
	* Randomly masks 15% of tokens in each sentence and asks the model to predict the missing word. Because if we only replace masked tokens with a special placeholder `[MASK]``, the special token would never be encountered during fine-tuning. Hence, BERT employed several heuristic tricks:
		* with 80% probability, replace the chosen words with `[MASK]`;
		* with 10% probability, replace with a random word;
		* with 10% probability, keep it the same.
	* The model only predicts the missing words, but it has no information on which words have been replaced or which words should be predicted. The output size is only 15% of the input size.
* Next Sentence Prediction
	* auxiliary task on training a binary classifier for telling whether one sentence is the next sentence of the other

{: .text-center}
![BERT input representation.]({{ site.url }}{{ site.baseurl }}/docs/images/llm-bert-input.png)
Fig. BERT input representation. Credits: From {% cite devlin2019bertpretrainingdeepbidirectional%}

**Embeddings in BERT**
* **WordPiece tokenization embeddings**: The WordPiece model was originally proposed for Japanese or Korean segmentation problems. Instead of using naturally split English word, they can be further divided into smaller sub-word units so that it is more effective to handle rare or unknown words.
* **Segment embeddings**: If the input contains two sentences, they have sentence A embeddings and sentence B embeddings respectively and they are separated by a special character `[SEP]`; Only sentence A embeddings are used if the input only contains one sentence.
* **Position embeddings**: Positional embeddings are learned rather than hard-coded.

**Model Types**
* **BERT Base**: 12 encoder layers, 768 hidden units (in feedforward), and 12 attention heads.
* **BERT Large**: 24 encoder layers, 1024 hidden units (in feedforward), and 24 attention heads.

{: .text-center}
![BERT downstream tasks.]({{ site.url }}{{ site.baseurl }}/docs/images/llm-bert-tasks.png)
Fig. Illustrations of Fine-tuning BERT on Different Task. Credits: From {% cite devlin2019bertpretrainingdeepbidirectional%}

### ALBERT: A Lite BERT

Proposed by {%cite lan2020albertlitebertselfsupervised%}, is a lightweight version of BERT, 1.7x faster in training with 18x fewer parameters.
* Factorized Embedding Parameterization: WordPiece tokenization embedding size E is configured to be the same as the hidden state size H. If we want to increase the model size ($$H$$), we need to increase the vocabulary size (V) as well. Using factorized embedding parameterization, the large vocabulary embedding matrix of size $$V \times H$$ is decomposed into $$V \times E$$ and $$E \times H$$. Given $$H >> E$$, factorization can result in greater parameter reduction.
* Cross-layer Parameter Sharing
* Sentence-Order Prediction (SOP): Replaces next sentence prediction (NSP) task of BERT by using (a) Positive sample: two consecutive segments from the same document, and (b) Negative sample: same as above, but the segment order is switched.

### GPT-2
Original Paper: {%cite radford2019language%}
* Direct suppressor of GPT, 1.5B parameters (10x more than the original GPT), 48 transformer layers, and it achieves SOTA results on 7 out of 8 tested language modeling datasets in a zero-shot transfer setting without any task-specific fine-tuning.
* The pre-training dataset contains 8 million Web pages collected by crawling qualified outbound links from Reddit.
* The pre-training task for GPT-2 is solely language modeling
* Tokenization: Uses BPE on UTF-8 Byte Sequences (4 Bytes for 1 character). GPT-2 prevents BPE from merging characters across categories (thus dog would not be merged with punctuations like., ! and ?). These tricks help increase the quality of the final byte segmentation.
* Involved only a few architectural changes - (a) layer normalization was moved to the input of each sub-block, similar to a residual unit of type “building block”, etc.

### RoBERTa: Robustly optimized BERT
{%cite liu2019robertarobustlyoptimizedbert%} proposed new recipe for training BERT by (a) training for longer with bigger batch size, (b) removing NSP task, (c) use longer sequences in training data format, i.e., use multiple sentences sampled contiguously to form longer segments, (d) change the masking pattern dynamically. Used BPE as in GPT-2. 

### T5: Text-to-Text Transfer Transformer

{%cite raffel2023exploringlimitstransferlearning%} adopts the natural language decathlon framework (in {%cite mccann2018naturallanguagedecathlonmultitask%} ) where many NLP tasks are translated into QA over a context.  Instead of an explicit QA format, T5 uses short task prefixes to distinguish task intentions and separately fine-tunes the model on every individual task.
The model is fine-tuned for each downstream task separately via “adapter layers” (add an extra layer for training) or “gradual unfreezing” (like ULMFit)


### GPT-3
{%cite brown2020languagemodelsfewshotlearners%} has the same architecture as GPT-2 but with 175B parameters (10x larger than GPT-2), 96 transformer layers.  In addition, GPT-3 uses alternating dense and locally banded sparse attention patterns, same as in sparse transformers. In order to fit such a huge model across multiple GPUs, GPT-3 is trained with partitions along both width and depth dimension.

### XLNet
{%cite yang2020xlnetgeneralizedautoregressivepretraining%}

Motivation: The Autoregressive (AR) model such as GPT and autoencoder (AE) model such as BERT are two most common ways for language modeling. However, each has their own disadvantages: AR does not learn the bidirectional context, which is needed by downstream tasks like reading comprehension and AE assumes masked positions are independent given all other unmasked tokens which oversimplifies the long context dependency.

Details are presented in {% cite weng2019LM%} but skipped since they seem wrong. 

### BART:  Bidirectional and AutoRegressive Transformer
{%cite lewis2019bartdenoisingsequencetosequencepretraining%} is a  denoising autoencoder to recover the original text from a randomly corrupted version jointly training BERT-like bidirectional encoder and GPT-like autoregressive decoder together. The best noising approach they discovered is text infilling and sentence shuffling (after experimenting with a variety of noising transformations).

{: .text-center}
![BART Comparison]({{ site.url }}{{ site.baseurl }}/docs/images/llm-bart.png)
Fig. A schematic comparison of BART with BERT {%cite devlin2019bertpretrainingdeepbidirectional%} and GPT {%cite radford2018improving%}. Credits: From {%cite lewis2019bartdenoisingsequencetosequencepretraining%} 

### ELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately
{%cite clark2020electrapretrainingtextencoders%} aims to improve the pre-training efficiency, which frames the language modeling as a discrimination task instead of generation task. Proposes Replaced Token Detection (RTD).  Let’s randomly sample k positions to be masked. Each selected token in the original text is replaced by a plausible alternative predicted by a small language model, known as the generator G. The discriminator D predicts whether each token is original or replaced. The loss for the generator is the negative log-likelihood just as in other language models. The loss for the discriminator is the cross-entropy. Note that the generator is not adversarially trained to fool the discriminator but simply to optimize the NLL, since their experiments show negative results. After pretraining the generator is discarded and only the ELECTRA discriminator is fine-tuned further for downstream tasks.

* more beneficial to only share the embeddings between generator & discriminator while using a small generator (1/4 to 1/2 the discriminator size), rather than sharing all the weights (i.e. two models have to be the same size then)
* joint training of the generator and discriminator works better than two-stage training of each alternatively


## Reading List


| Title                                          |  Topic       |   Comments                                                   |
| --------------------------------------------   | ------------ | ------------------------------------------------------------ |
| [The Illustrated BERT](https://jalammar.github.io/illustrated-bert/) {% cite alammar-illustratedtbert%}| LMs | Good Short Overview
| [Generalized Language Models by Lilian Weng](https://lilianweng.github.io/posts/2019-01-31-lm/) {% cite weng2019LM%}| LMs | Great overview of BERT and its successors

## References


{% bibliography --cited %}