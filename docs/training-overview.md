---
layout: single
title: Training Overview
permalink: /docs/training-overview
toc: true
---

## Overview

Typical training pipeline involves (a) pre-training, (b) supervised fine-tuning i.e., SFT and (c) RLHF or Alignment.

{: .text-center}
![LLM Training Pipeline]({{ site.url }}{{ site.baseurl }}/docs/images/training-overview-method.png)
Fig. Typical LLM Training Pipeline. Image Credits {% cite chip-rlhf%}

### Pre-Training
**Pre-training** is performed on very large low-quality data. The rate of training dataset size growth is much faster than the rate of new data being generated {%cite villalobos2024rundatalimitsllm%}.

* GPT-3’s dataset (OpenAI): 0.5 trillion tokens  
* Gopher’s dataset (DeepMind): 1 trillion tokens  
* RedPajama (Together): 1.2 trillion tokens  
* LLaMa’s dataset (Meta): 1.4 trillion tokens

1 Trillion tokens are equivalent to approximately 15 million books.


### Supervised Fine-Tuning (SFT)

In **SFT**, the examples are high-quality data, following the format (prompt, response) and are called demonstration data. OpenAI calls supervised finetuning behavior cloning. Data scale: 10k \- 100k (prompt, response) pairs. SFT allows models to better adhere to specific instructions. 

* InstructGPT: ~14.5k pairs (13k from labelers +1.5k from customers)  
* Alpaca: 52K ChatGPT instructions  
* Databricks’ Dolly-15k: ~15k pairs, created by Databricks employees  
* OpenAssistant: 161k messages in 10k conversations $$\rightarrow$$ approximately 88k pairs  
* Dialogue-fine tuned Gopher: ~5 billion tokens, which is estimated to be in the order of 10M messages. However, these are filtered out using heuristics from the Internet, so not of the highest quality.

#### Finetuning Methods
* Instruction Finetuning:  to get openly available LLMs to better follow instructions or specialize these LLMs on subsets or new instructions
* Continually Pre-training: taking in new knowledge
* Proxy Tuning: Finetuning LLMs without altering their weights



### RLHF or Alignment

**The idea**: what if we have a scoring function that, if given a prompt and a response, outputs a score for how good that response is? Then we use this scoring function to further train our LLMs towards giving responses with high scores. That’s exactly what RLHF does. RLHF consists of two parts:

* Train a reward model to act as a scoring function.  
* Optimize LLM to generate responses for which the reward model will give high scores.

Data scale: 100K - 1M examples

* InstructGPT: 50k prompts. Each prompt has 4 to 9 responses, forming between 6 and 36 pairs of (winning_response, losing_response). This means between 300K and 1.8M training examples in the format of (prompt, winning_response, losing_response).  
* Constitutional AI, which is suspected to be the backbone of Claude (Anthropic): 318K comparisons – 135K generated by humans, and 183K generated by AI. Anthropic has an older version of their data open-sourced (hh-rlhf), which consists of roughly 170K comparisons.

Alignment step hones the LLMs to respond more helpfully and safely to user prompts. In some cases, RLHF step 1 (like InstructGPT) includes SFT.

## Insights into training LLMs

{%cite biderman2023pythiasuiteanalyzinglarge%} released 8 LLMs (from 70M to 12B parameters) along with training details, analysis and insights:
* Does pretraining on duplicated data (i.e., training for >1 epoch) make a difference? It turns out that deduplication does not benefit or hurt performance.
* Does training order influence memorization? Unfortunately, it turns out that it does not. "Unfortunately," because if this was true, we could mitigate undesirable verbatim memorization issues by reordering the training data.
* Does pretrained term frequency influence task performance? Yes, few-shot accuracy tends to be higher for terms that occur more frequently.
* Does increasing the batch size affect training efficiency and model convergence? Doubling the batch size halves the training time but doesn't hurt convergence.

## Instruction Pre-training

{%cite cheng2024instructionpretraininglanguagemodels%} investigate whether LLM pretraining can be made more efficient by including synthetic instruction-response pairs instead of just raw text. Used Instruction Synthesizer to create the text token stream along with the instructions and responses. 

**Creating Alignment Data from Scratch**
MagPie {%cite xu2024magpiealignmentdatasynthesis%} proposed a hack to generate high-quality instruction finetuning dataset. It prompts the Llama3-8B with a pre-query template (<start_header_id>user<end_header_id>) as input, resulting in an instruction, and the instruction is further fed to Llama3-8B to get the response. 



## Reading List


| Title                                          |  Topic       |   Comments                                                   |
| --------------------------------------------   | ------------ | ------------------------------------------------------------ |
| [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html) {% cite chip-rlhf%}| Training | Great intro to pre-training, SFT and RMs
| [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives) {% cite seb-rlhf%}| Training | Good overview of RLHF
| [LLM Alignment by Aman.AI](https://aman.ai/primers/ai/llm-alignment/) {% cite aman-alignment%}| Training | Very thorough, not all topics are useful at the current date
| [Predictive Human Preference: From Model Ranking to Model Routing](https://huyenchip.com/2024/02/28/predictive-human-preference.html) {% cite chip-human-preference%}| Training | Basics of Model evaluation, routing and ranking. Other items like predictive human preference experiments can be ignored.
| [Aligning language models to follow instructions by OpenAI](https://openai.com/index/instruction-following/) {% cite openai-instruction-following%}| Training | 
| [Reinforcement Learning for Language Models](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81) {% cite yoavg-rl-llm%}| Training | 
| [Instruction Pretraining LLMs](https://magazine.sebastianraschka.com/p/instruction-pretraining-llms) {% cite seb-instruction-pretraining%}| Training | 

## References


{% bibliography --cited %}