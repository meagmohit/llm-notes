@article{doe2021,
  author = {John Doe},
  title = {The Impact of Jekyll Scholar on Academic Writing},
  journal = {Journal of Open Source},
  year = {2021},
  volume = {5},
  number = {2},
  pages = {123-134},
}

@book{smith2019,
  author = {Jane Smith},
  title = {Jekyll for Beginners},
  publisher = {Tech Books Publishing},
  year = {2019},
}

@misc{example-link1,
  author = {Author, A.},
  year = {2001},
  title = {Alpha},
  url = {www.tex.stackexchange.com},
}

@misc{example-link2,
  author = {Ingo Lütkebohle},
  title = {{BWorld Robot Control Software}},
  howpublished = "\url{http://aiweb.techfak.uni-bielefeld.de/content/bworld-robot-control-software/}",
  year = {2008}, 
  note = "[Online; accessed 19-July-2008]"
}

% Main 

@misc{peters2018deepcontextualizedwordrepresentations,
      title={Deep contextualized word representations}, 
      author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
      year={2018},
      eprint={1802.05365},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1802.05365}, 
}

@misc{bahdanau2016neuralmachinetranslationjointly,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.0473}, 
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@misc{britz2017massiveexplorationneuralmachine,
      title={Massive Exploration of Neural Machine Translation Architectures}, 
      author={Denny Britz and Anna Goldie and Minh-Thang Luong and Quoc Le},
      year={2017},
      eprint={1703.03906},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1703.03906}, 
}

@article{Chadha2020DistilledTransformers,
  title   = {Transformers},
  author  = {Chadha, Aman},
  journal = {Distilled AI},
  year    = {2020},
  url   = {https://aman.ai}
}

@misc{alammar-illustratedtransformer,
  author = {Alammar, J.},
  year = {2018},
  title = {The Illustrated Transformer [Blog post]},
  url = {https://jalammar.github.io/illustrated-transformer/},
}

@article{weng2020transformer,
  title   = "The Transformer Family",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2020",
  month   = "Apr",
  url     = "https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/"
}

@article{weng2023transformer,
  title   = "The Transformer Family Version 2.0",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2023",
  month   = "Jan",
  url     = "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/"
}

@misc{3b1b-transformers,
  author = {3Blue1Brown},
  year = {2024},
  title = {Attention in transformers, visually explained [Video]},
  url = {https://www.youtube.com/watch?v=eMlx5fFNoYc&ab_channel=3Blue1Brown},
}

@misc{he2015deepresiduallearningimage,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}

@article{yan2023attention,
  title   = {Some Intuition on Attention and the Transformer},
  author  = {Yan, Ziyou},
  journal = {eugeneyan.com},
  year    = {2023},
  month   = {May},
  url     = {https://eugeneyan.com/writing/attention/}
}

@misc{dai2019transformerxlattentivelanguagemodels,
      title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}, 
      author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
      year={2019},
      eprint={1901.02860},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1901.02860}, 
}

@misc{child2019generatinglongsequencessparse,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.10509}, 
}

@misc{kitaev2020reformerefficienttransformer,
      title={Reformer: The Efficient Transformer}, 
      author={Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
      year={2020},
      eprint={2001.04451},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.04451}, 
}

@misc{gomez2017reversibleresidualnetworkbackpropagation,
      title={The Reversible Residual Network: Backpropagation Without Storing Activations}, 
      author={Aidan N. Gomez and Mengye Ren and Raquel Urtasun and Roger B. Grosse},
      year={2017},
      eprint={1707.04585},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1707.04585}, 
}

@misc{dehghani2019universaltransformers,
      title={Universal Transformers}, 
      author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Łukasz Kaiser},
      year={2019},
      eprint={1807.03819},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1807.03819}, 
}

@misc{parisotto2019stabilizingtransformersreinforcementlearning,
      title={Stabilizing Transformers for Reinforcement Learning}, 
      author={Emilio Parisotto and H. Francis Song and Jack W. Rae and Razvan Pascanu and Caglar Gulcehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and Matthew M. Botvinick and Nicolas Heess and Raia Hadsell},
      year={2019},
      eprint={1910.06764},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.06764}, 
}

@misc{beltagy2020longformerlongdocumenttransformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.05150}, 
}

@misc{child2019generatinglongsequencessparse,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.10509}, 
}

@misc{seb-10aipapers2023,
  author = {Sebastian Raschka},
  year = {2023},
  title = {Ten Noteworthy AI Research Papers of 2023 [Blog]},
  url = {https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023},
}

@misc{seb-instrction-pretraining,
  author = {Sebastian Raschka},
  year = {2024},
  title = {Instruction Pretraining LLMs [Blog]},
  url = {https://magazine.sebastianraschka.com/p/instruction-pretraining-llms},
}

@misc{ainslie2023gqatraininggeneralizedmultiquery,
      title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}, 
      author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebrón and Sumit Sanghai},
      year={2023},
      eprint={2305.13245},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13245}, 
}

@misc{gehring2017convolutionalsequencesequencelearning,
      title={Convolutional Sequence to Sequence Learning}, 
      author={Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N. Dauphin},
      year={2017},
      eprint={1705.03122},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1705.03122}, 
}

@misc{alrfou2018characterlevellanguagemodelingdeeper,
      title={Character-Level Language Modeling with Deeper Self-Attention}, 
      author={Rami Al-Rfou and Dokook Choe and Noah Constant and Mandy Guo and Llion Jones},
      year={2018},
      eprint={1808.04444},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1808.04444}, 
}

@misc{shaw2018selfattentionrelativepositionrepresentations,
      title={Self-Attention with Relative Position Representations}, 
      author={Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
      year={2018},
      eprint={1803.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1803.02155}, 
}

@misc{su2023roformerenhancedtransformerrotary,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.09864}, 
}

@misc{smith2023convnetsmatchvisiontransformers,
      title={ConvNets Match Vision Transformers at Scale}, 
      author={Samuel L. Smith and Andrew Brock and Leonard Berrada and Soham De},
      year={2023},
      eprint={2310.16764},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2310.16764}, 
}

@misc{mccann2018learnedtranslationcontextualizedword,
      title={Learned in Translation: Contextualized Word Vectors}, 
      author={Bryan McCann and James Bradbury and Caiming Xiong and Richard Socher},
      year={2018},
      eprint={1708.00107},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1708.00107}, 
}

@misc{alammar-illustratedtbert,
  author = {Alammar, J.},
  year = {2018},
  title = {The Illustrated Bert [Blog post]},
  url = {https://jalammar.github.io/illustrated-bert/},
}

@article{weng2019LM,
  title   = "Generalized Language Models",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2019",
  url     = "https://lilianweng.github.io/posts/2019-01-31-lm/"
}

@misc{mccann2018learnedtranslationcontextualizedword,
      title={Learned in Translation: Contextualized Word Vectors}, 
      author={Bryan McCann and James Bradbury and Caiming Xiong and Richard Socher},
      year={2018},
      eprint={1708.00107},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1708.00107}, 
}

@misc{peters2017semisupervisedsequencetaggingbidirectional,
      title={Semi-supervised sequence tagging with bidirectional language models}, 
      author={Matthew E. Peters and Waleed Ammar and Chandra Bhagavatula and Russell Power},
      year={2017},
      eprint={1705.00108},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1705.00108}, 
}

@misc{peters2018deepcontextualizedwordrepresentations,
      title={Deep contextualized word representations}, 
      author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
      year={2018},
      eprint={1802.05365},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1802.05365}, 
}

@misc{clark2018semisupervisedsequencemodelingcrossview,
      title={Semi-Supervised Sequence Modeling with Cross-View Training}, 
      author={Kevin Clark and Minh-Thang Luong and Christopher D. Manning and Quoc V. Le},
      year={2018},
      eprint={1809.08370},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1809.08370}, 
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}

@misc{howard2018universallanguagemodelfinetuning,
      title={Universal Language Model Fine-tuning for Text Classification}, 
      author={Jeremy Howard and Sebastian Ruder},
      year={2018},
      eprint={1801.06146},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1801.06146}, 
}

@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{lan2020albertlitebertselfsupervised,
      title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, 
      author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
      year={2020},
      eprint={1909.11942},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.11942}, 
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{liu2019robertarobustlyoptimizedbert,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@misc{raffel2023exploringlimitstransferlearning,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683}, 
}

@misc{mccann2018naturallanguagedecathlonmultitask,
      title={The Natural Language Decathlon: Multitask Learning as Question Answering}, 
      author={Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},
      year={2018},
      eprint={1806.08730},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1806.08730}, 
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{yang2020xlnetgeneralizedautoregressivepretraining,
      title={XLNet: Generalized Autoregressive Pretraining for Language Understanding}, 
      author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
      year={2020},
      eprint={1906.08237},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1906.08237}, 
}

@misc{lewis2019bartdenoisingsequencetosequencepretraining,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.13461}, 
}

@misc{clark2020electrapretrainingtextencoders,
      title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}, 
      author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
      year={2020},
      eprint={2003.10555},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2003.10555}, 
}

@misc{chip-multimodal,
  author = {Chip Huyen},
  year = {2023},
  title = {Multimodality and Large Multimodal Models (LMMs) [Blog]},
  url = {https://huyenchip.com/2023/10/10/multimodal.html},
}

@article{weng2022vlm,
  title   = "Generalized Visual Language Models",
  author  = "Weng, Lilian",
  journal = "Lil'Log",
  year    = "2022",
  month   = "Jun",
  url     = "https://lilianweng.github.io/posts/2022-06-09-vlm/"
}

@article{Chadha2020DistilledVisionLanguageModels,
  title   = {Vision Language Models},
  author  = {Chadha, Aman},
  journal = {Distilled AI},
  year    = {2020},
  note    = {\url{https://aman.ai}}
}
@misc{nichol2022glidephotorealisticimagegeneration,
      title={GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models}, 
      author={Alex Nichol and Prafulla Dhariwal and Aditya Ramesh and Pranav Shyam and Pamela Mishkin and Bob McGrew and Ilya Sutskever and Mark Chen},
      year={2022},
      eprint={2112.10741},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.10741}, 
}

@misc{ramesh2021zeroshottexttoimagegeneration,
      title={Zero-Shot Text-to-Image Generation}, 
      author={Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
      year={2021},
      eprint={2102.12092},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2102.12092}, 
}

@misc{yuan2021florencenewfoundationmodel,
      title={Florence: A New Foundation Model for Computer Vision}, 
      author={Lu Yuan and Dongdong Chen and Yi-Ling Chen and Noel Codella and Xiyang Dai and Jianfeng Gao and Houdong Hu and Xuedong Huang and Boxin Li and Chunyuan Li and Ce Liu and Mengchen Liu and Zicheng Liu and Yumao Lu and Yu Shi and Lijuan Wang and Jianfeng Wang and Bin Xiao and Zhen Xiao and Jianwei Yang and Michael Zeng and Luowei Zhou and Pengchuan Zhang},
      year={2021},
      eprint={2111.11432},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2111.11432}, 
}

@misc{yu2022cocacontrastivecaptionersimagetext,
      title={CoCa: Contrastive Captioners are Image-Text Foundation Models}, 
      author={Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},
      year={2022},
      eprint={2205.01917},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2205.01917}, 
}

@misc{alayrac2022flamingovisuallanguagemodel,
      title={Flamingo: a Visual Language Model for Few-Shot Learning}, 
      author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
      year={2022},
      eprint={2204.14198},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.14198}, 
}

@misc{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}


@misc{luo2022frustratinglysimpleapproachendtoend,
      title={A Frustratingly Simple Approach for End-to-End Image Captioning}, 
      author={Ziyang Luo and Yadong Xi and Rongsheng Zhang and Jing Ma},
      year={2022},
      eprint={2201.12723},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2201.12723}, 
}

@misc{chen2022visualgptdataefficientadaptationpretrained,
      title={VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning}, 
      author={Jun Chen and Han Guo and Kai Yi and Boyang Li and Mohamed Elhoseiny},
      year={2022},
      eprint={2102.10407},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2102.10407}, 
}

@misc{li2019visualbertsimpleperformantbaseline,
      title={VisualBERT: A Simple and Performant Baseline for Vision and Language}, 
      author={Liunian Harold Li and Mark Yatskar and Da Yin and Cho-Jui Hsieh and Kai-Wei Chang},
      year={2019},
      eprint={1908.03557},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1908.03557}, 
}
@misc{wang2022simvlmsimplevisuallanguage,
      title={SimVLM: Simple Visual Language Model Pretraining with Weak Supervision}, 
      author={Zirui Wang and Jiahui Yu and Adams Wei Yu and Zihang Dai and Yulia Tsvetkov and Yuan Cao},
      year={2022},
      eprint={2108.10904},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2108.10904}, 
}
@misc{aghajanyan2022cm3causalmaskedmultimodal,
      title={CM3: A Causal Masked Multimodal Model of the Internet}, 
      author={Armen Aghajanyan and Bernie Huang and Candace Ross and Vladimir Karpukhin and Hu Xu and Naman Goyal and Dmytro Okhonko and Mandar Joshi and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer},
      year={2022},
      eprint={2201.07520},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.07520}, 
}

@misc{jia2021scalingvisualvisionlanguagerepresentation,
      title={Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision}, 
      author={Chao Jia and Yinfei Yang and Ye Xia and Yi-Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yunhsuan Sung and Zhen Li and Tom Duerig},
      year={2021},
      eprint={2102.05918},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2102.05918}, 
}


@misc{tsimpoukelli2021multimodalfewshotlearningfrozen,
      title={Multimodal Few-Shot Learning with Frozen Language Models}, 
      author={Maria Tsimpoukelli and Jacob Menick and Serkan Cabi and S. M. Ali Eslami and Oriol Vinyals and Felix Hill},
      year={2021},
      eprint={2106.13884},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2106.13884}, 
}
@misc{mokady2021clipcapclipprefiximage,
      title={ClipCap: CLIP Prefix for Image Captioning}, 
      author={Ron Mokady and Amir Hertz and Amit H. Bermano},
      year={2021},
      eprint={2111.09734},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2111.09734}, 
}

@misc{lu2019vilbertpretrainingtaskagnosticvisiolinguistic,
      title={ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks}, 
      author={Jiasen Lu and Dhruv Batra and Devi Parikh and Stefan Lee},
      year={2019},
      eprint={1908.02265},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1908.02265}, 
}
@misc{zeng2022socraticmodelscomposingzeroshot,
      title={Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language}, 
      author={Andy Zeng and Maria Attarian and Brian Ichter and Krzysztof Choromanski and Adrian Wong and Stefan Welker and Federico Tombari and Aveek Purohit and Michael Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Pete Florence},
      year={2022},
      eprint={2204.00598},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.00598}, 
}
@misc{yang2022empiricalstudygpt3fewshot,
      title={An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA}, 
      author={Zhengyuan Yang and Zhe Gan and Jianfeng Wang and Xiaowei Hu and Yumao Lu and Zicheng Liu and Lijuan Wang},
      year={2022},
      eprint={2109.05014},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2109.05014}, 
}

@misc{su2022languagemodelsseeplugging,
      title={Language Models Can See: Plugging Visual Controls in Text Generation}, 
      author={Yixuan Su and Tian Lan and Yahui Liu and Fangyu Liu and Dani Yogatama and Yan Wang and Lingpeng Kong and Nigel Collier},
      year={2022},
      eprint={2205.02655},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2205.02655}, 
}

@misc{zellers2021merlotmultimodalneuralscript,
      title={MERLOT: Multimodal Neural Script Knowledge Models}, 
      author={Rowan Zellers and Ximing Lu and Jack Hessel and Youngjae Yu and Jae Sung Park and Jize Cao and Ali Farhadi and Yejin Choi},
      year={2021},
      eprint={2106.02636},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2106.02636}, 
}

@misc{gwern-scaling,
  author = {Gwern},
  year = {2022},
  title = {The Scaling Hypothesis [Blog]},
  url = {https://gwern.net/scaling-hypothesis},
}

@misc{hackernoon-scaling,
  author = {Ravi Mandliya},
  year = {2024},
  title = {Scaling Laws in Large Language Models [Blog]},
  url = {https://hackernoon.com/scaling-laws-in-large-language-models},
}

@article{valiant1984theory,
  title={A theory of the learnable},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={27},
  number={11},
  pages={1134--1142},
  year={1984},
  publisher={ACM New York, NY, USA}
}

@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@misc{hoffmann2022trainingcomputeoptimallargelanguage,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}